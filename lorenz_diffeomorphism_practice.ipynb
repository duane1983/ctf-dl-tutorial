{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping time-delay embedding of partial measurements to full-state attractor\n",
    "\n",
    "For access on Google Colab, use this link: https://colab.research.google.com/drive/1CNh5cUk6OH1bHposHS2u3P4UUk6reS70?usp=sharing\n",
    "\n",
    "WARNING: this notebook is has some bugs and *problems*!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "# %matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "# Lorenz System\n",
    "\n",
    "def lorenz(Z, t, sigma, rho, beta):\n",
    "    x, y, z = Z\n",
    "    return [sigma*(y-x), x*(rho-z)-y, x*y-beta*z]\n",
    "\n",
    "x0 = [1, 1, 1]\n",
    "t = np.linspace(0, 100, 100000)\n",
    "sol = odeint(lorenz, x0, t, args=(10, 28, 8/3))\n",
    "\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(sol[:,0], sol[:,1], sol[:,2], lw=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build hankel matrix\n",
    "\n",
    "delays = 100\n",
    "sysdim = sol.shape[1]\n",
    "dt = t[1] - t[0]\n",
    "\n",
    "def build_hankel(x, delays):\n",
    "    H = np.zeros((delays, len(x)-delays))\n",
    "    for i in range(delays):\n",
    "        H[i, :] = x[i: i+H.shape[1]]\n",
    "    \n",
    "    return H\n",
    "\n",
    "measurements = sol[:, 0]\n",
    "H = build_hankel(measurements, delays)\n",
    "print('H_shape: ', H.shape)\n",
    "print('dt : {}'.format(dt*delays))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would like to find a mapping between the time delays $\\mathbf{\\hat{y}} = [x_i, x_{i+1}, \\ldots, x_{i+n-1}]$ and the full state system $\\mathbf z_i = [x_i, y_i, z_i]$\n",
    "\n",
    "$$ \\mathbf z = \\mathbf f(\\hat{\\mathbf y}) $$\n",
    "\n",
    "Can we approximate the function $\\mathbf f$ by a neural network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define model\n",
    "class network(nn.Module):\n",
    "    def __init__(self, input_dim=100, output_dim=3):\n",
    "        super(network, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim \n",
    "        self.l1 = nn.Linear(self.input_dim, 50)\n",
    "        self.l2 = nn.Linear(50, 10)\n",
    "        self.l4 = nn.Linear(10, self.output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.l1(x))\n",
    "        x = F.relu(self.l2(x))\n",
    "        x = F.relu(self.l4(x))\n",
    "        return x\n",
    "\n",
    "model = network(delays, sysdim)\n",
    "print(model)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([79920, 100])\n",
      "torch.Size([19980, 100])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data\n",
    "X = H.T\n",
    "y = sol[:X.shape[0], :]\n",
    "\n",
    "# Split into training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=.8, shuffle=True)\n",
    "\n",
    "# Specify the device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Turn numpy arrays into PyTorch Tensors\n",
    "x_train = torch.tensor(x_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "trainset = TensorDataset(x_train, y_train)\n",
    "testset = TensorDataset(x_test, y_test)\n",
    "\n",
    "# Prepare data for training\n",
    "bs = 30\n",
    "trainset_dl = DataLoader(trainset, batch_size=bs, shuffle=False)\n",
    "testset_dl = DataLoader(testset, batch_size=bs, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizer and loss function\n",
    "optim = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train_loop(model, loss_fn, optim, trainset_dl, epochs=100):\n",
    "    # Loop through epochs\n",
    "    for i in range(epochs):\n",
    "        # Loop through batches\n",
    "        for xb, yb in trainset_dl:\n",
    "            \n",
    "            # 1. Forward pass\n",
    "            y_pred = model(xb)\n",
    "\n",
    "            # 2. Compute loss\n",
    "            loss = loss_fn(y_pred, yb)\n",
    "\n",
    "            # 3. Backward pass (compute gradients)\n",
    "            loss.backward()\n",
    "\n",
    "            # 4. Apply optimization step\n",
    "            optim.step()\n",
    "\n",
    "            # 5. Set gradients to zero \n",
    "            # (Pytorch accumulates gradients otherwise) \n",
    "            optim.zero_grad()\n",
    "\n",
    "        print('Epoch: {}, Loss: {}'.format(i, loss))\n",
    "\n",
    "train_loop(model, loss_fn, optim, trainset_dl, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "\n",
    "# Get predictions\n",
    "with torch.no_grad():\n",
    "    y_pred = model(torch.tensor(H.T, dtype=torch.float32))\n",
    "\n",
    "# Plot predictions\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot(y_pred[:,0], y_pred[:,1], y_pred[:,2], lw=.5, label='Predictions')\n",
    "ax.plot(y[:,0], y[:,1], y[:,2], lw=.5, label='True')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1\n",
    "Fill in the blanks to perform both training and testing on every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "def train_step(model, loss_fn, optim, trainset_dl):\n",
    "    ### Write a training step\n",
    "\n",
    "    return loss\n",
    "\n",
    "def test_step(model, loss_fn, optim, testset_dl):\n",
    "    ### And a test step\n",
    "\n",
    "    return avg_loss \n",
    "\n",
    "def fit(model, loss_fn, optim, trainset_dl, testset_dl, epochs=100):\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "    for i in range(epochs):\n",
    "        ### Perform both train and test steps every epoch \n",
    "\n",
    "        print('Epoch: {}, Train loss: {}, Test loss: {}'.format(i, train_loss[-1], test_loss[-1]))\n",
    "    return train_loss, test_loss\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "model2 = network(delays, sysdim)\n",
    "optim2 = torch.optim.Adam(model2.parameters(), lr=1e-3)\n",
    "train_loss, test_loss = fit(model2, loss_fn, optim2, trainset_dl, testset_dl, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "### Plot training and test loss as a function of epochs\n",
    "\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What about the dominant eigen-time-modes of the delay embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find linear modes\n",
    "high_dimensional_time_series = \n",
    "\n",
    "u, s, v = np.linalg.svd(high_dimensional_time_series, full_matrices=False)\n",
    "modes = v.T\n",
    "\n",
    "# Plot modes\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.set_title('Linear modes')\n",
    "ax.plot(modes[:,0], modes[:,1], modes[:,2], lw=.5, label='Modes')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Build an autoencoder to find the non-linear dominant modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class encoder(nn.Module):\n",
    "    def __init__(self, input_dim=100, output_dim=3):\n",
    "        super(encoder, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim \n",
    "        self.l1 = \n",
    "        self.l2 = \n",
    "        self.l3 = \n",
    "        self.l4 = \n",
    "                   \n",
    "    def forward(self, x):\n",
    "        x = \n",
    "        return x\n",
    "\n",
    "### Define decoder\n",
    "\n",
    "\n",
    "class autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim=100, latent_dim=3):\n",
    "        super(autoencoder, self).__init__()\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        return x\n",
    "\n",
    "AEmodel = autoencoder(input_dim=100, latent_dim=3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for training\n",
    "bs = 30\n",
    "AEtrainset = TensorDataset(x_train, x_train)\n",
    "AEtestset = TensorDataset(x_test, x_test)\n",
    "\n",
    "# Prepare data for training\n",
    "AEtrainset_dl = DataLoader(AEtrainset, batch_size=bs, shuffle=False)\n",
    "AEtestset_dl = DataLoader(AEtestset, batch_size=bs, shuffle=False)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimAE = torch.optim.Adam(AEmodel.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train_loss, test_loss = fit(AEmodel, loss_fn, optimAE, AEtrainset_dl, AEtestset_dl, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and test losses\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(train_loss, label='Train loss')\n",
    "ax.plot(test_loss, label='Test loss')\n",
    "ax.set_xlabel('Epochs')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "hankel = torch.tensor(H.T, dtype=torch.float32)\n",
    "latent_z = AEmodel.encoder(hankel).detach().numpy()\n",
    "\n",
    "# Plot predictions\n",
    "fig = plt.figure(figsize=(3, 3))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(latent_z[:,0], latent_z[:,1], latent_z[:,2], lw=.5, label='Latent space')\n",
    "ax.plot(y[:,0], y[:,1], y[:,2], lw=.5, label='Original Lorenz')\n",
    "ax.plot(modes[:,0], modes[:,1], modes[:,2], lw=.5, label='Linear modes')\n",
    "\n",
    "ax.set_title('Nonlinear Modes')\n",
    "ax.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "AEmodel.encoder.l1.register_forward_hook(get_activation('l1'))\n",
    "output_l1 = AEmodel(hankel)\n",
    "l1_output = activation['l1'].detach().numpy()\n",
    "\n",
    "AEmodel.encoder.l2.register_forward_hook(get_activation('l2'))\n",
    "output_l2 = AEmodel(hankel)\n",
    "l2_output = activation['l2'].detach().numpy()\n",
    "\n",
    "AEmodel.encoder.l3.register_forward_hook(get_activation('l3'))\n",
    "output_l3 = AEmodel(hankel)\n",
    "l3_output = activation['l3'].detach().numpy()\n",
    "\n",
    "output_list = [l1_output, l2_output, l3_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99900, 50)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of layer outputs\n",
    "l1_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, out in enumerate(output_list):\n",
    "    # SVD every output layer and plot\n",
    "    u, s, v = np.linalg.svd(output_list[i].T, full_matrices=False)\n",
    "    modes = v.T\n",
    "    fig = plt.figure(figsize=(3, 3))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.set_title('Layer: {}'.format(i+1))\n",
    "    ax.plot(modes[:,0], modes[:,1], modes[:,2], lw=.5, label='Modes')\n",
    "    ax.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(AEmodel.state_dict(), 'AEmodel.pt')\n",
    "\n",
    "# Load model\n",
    "AEmodel2 = autoencoder(input_dim=100, latent_dim=3)\n",
    "AEmodel2.load_state_dict(torch.load('AEmodel.pt'))\n",
    "AEmodel2.eval()\n",
    "\n",
    "# print and compare first parameter of AE model and loaded model\n",
    "print(AEmodel.encoder.l1.weight[0])\n",
    "print(AEmodel.encoder.l1.weight[0] == AEmodel2.encoder.l1.weight[0])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you build a predictor that integrates the time series in time?\n",
    "\n",
    "$$ \\mathbf z_{i+1} = \\mathbf f(\\mathbf z_{i}) $$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1cec93c3674bfd1afa4b15c70c9d9dc0c8affa484b58136f14c49e79b9d4b788"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
